/**
 * Silero VAD (Voice Activity Detection) Service
 * Removes silence from audio before transcription for faster processing
 * Using ONNX model that works in browsers (~1MB)
 */

import { env, AutoProcessor, AutoModelForAudioFrameClassification } from '@xenova/transformers';

// Configure for browser environment
env.allowRemoteModels = true;
env.remoteURL = "https://huggingface.co/";

/**
 * Silero VAD Service for browser-based voice activity detection
 */
export class SileroVADService {
  constructor() {
    this.processor = null;
    this.model = null;
    this.isLoading = false;
    this.isLoaded = false;
    
    // VAD configuration
    this.config = {
      // Minimum speech duration to keep (seconds)
      minSpeechDuration: 0.25,
      // Minimum silence duration to remove (seconds)
      minSilenceDuration: 0.5,
      // Threshold for speech detection (0-1)
      speechThreshold: 0.5,
      // Frame size in samples (for 16kHz audio)
      frameSize: 512,
      // Sample rate
      sampleRate: 16000,
    };
  }
  
  /**
   * Initialize the VAD model
   */
  async initialize() {
    if (this.isLoaded || this.isLoading) return;
    
    this.isLoading = true;
    
    try {
      console.log('ðŸŽ¤ Loading Silero VAD model...');
      
      // Load the Silero VAD model from HuggingFace
      // Using the ONNX version optimized for browsers
      this.processor = await AutoProcessor.from_pretrained('onnx-community/silero-vad');
      this.model = await AutoModelForAudioFrameClassification.from_pretrained(
        'onnx-community/silero-vad',
        {
          quantized: true, // Use quantized model for smaller size
        }
      );
      
      this.isLoaded = true;
      console.log('âœ… Silero VAD model loaded successfully');
      
    } catch (error) {
      console.error('Failed to load Silero VAD:', error);
      
      // Fallback: Try alternative VAD model
      try {
        console.log('Trying alternative VAD model...');
        this.model = await AutoModelForAudioFrameClassification.from_pretrained(
          'Xenova/silero-vad',
          {
            quantized: true,
          }
        );
        this.isLoaded = true;
        console.log('âœ… Alternative VAD model loaded');
      } catch (fallbackError) {
        console.error('VAD initialization failed:', fallbackError);
        throw new Error('Could not load VAD model');
      }
    } finally {
      this.isLoading = false;
    }
  }
  
  /**
   * Process audio to detect voice activity
   * @param {Float32Array|AudioBuffer} audio - Input audio
   * @returns {Object} Segments with speech and processed audio
   */
  async detectVoiceActivity(audio) {
    if (!this.isLoaded) {
      await this.initialize();
    }
    
    // Convert audio to Float32Array if needed
    const audioData = this.normalizeAudio(audio);
    
    // Split audio into frames
    const frames = this.createFrames(audioData);
    
    // Detect speech in each frame
    const speechProbabilities = await this.detectSpeechFrames(frames);
    
    // Find speech segments
    const segments = this.findSpeechSegments(speechProbabilities);
    
    // Extract only speech segments
    const processedAudio = this.extractSpeechSegments(audioData, segments);
    
    return {
      segments,
      processedAudio,
      originalDuration: audioData.length / this.config.sampleRate,
      processedDuration: processedAudio.length / this.config.sampleRate,
      reductionPercent: ((1 - processedAudio.length / audioData.length) * 100).toFixed(1),
    };
  }
  
  /**
   * Normalize audio to Float32Array
   */
  normalizeAudio(audio) {
    if (audio instanceof Float32Array) {
      return audio;
    }
    
    if (audio instanceof AudioBuffer) {
      // Get the first channel
      return audio.getChannelData(0);
    }
    
    if (audio instanceof ArrayBuffer || audio instanceof Uint8Array) {
      // Convert to Float32Array
      const dataView = new DataView(
        audio instanceof Uint8Array ? audio.buffer : audio
      );
      const float32 = new Float32Array(dataView.byteLength / 2);
      
      for (let i = 0; i < float32.length; i++) {
        const int16 = dataView.getInt16(i * 2, true);
        float32[i] = int16 / 32768.0;
      }
      
      return float32;
    }
    
    throw new Error('Unsupported audio format');
  }
  
  /**
   * Create frames from audio data
   */
  createFrames(audioData) {
    const frames = [];
    const frameSize = this.config.frameSize;
    const hopSize = Math.floor(frameSize / 2); // 50% overlap
    
    for (let i = 0; i <= audioData.length - frameSize; i += hopSize) {
      frames.push(audioData.slice(i, i + frameSize));
    }
    
    return frames;
  }
  
  /**
   * Detect speech in frames using the model
   */
  async detectSpeechFrames(frames) {
    if (!this.model) {
      // Fallback to simple energy-based VAD
      return this.simpleVAD(frames);
    }
    
    try {
      // Process frames through the model
      const probabilities = [];
      
      for (const frame of frames) {
        // Run inference on the frame
        const inputs = await this.processor(frame, {
          sampling_rate: this.config.sampleRate,
          return_tensors: 'pt',
        });
        
        const output = await this.model(inputs);
        const speechProb = output.logits[0][1]; // Speech probability
        
        probabilities.push(speechProb);
      }
      
      return probabilities;
      
    } catch (error) {
      console.warn('Model inference failed, using fallback VAD:', error);
      return this.simpleVAD(frames);
    }
  }
  
  /**
   * Simple energy-based VAD fallback
   */
  simpleVAD(frames) {
    const probabilities = [];
    
    for (const frame of frames) {
      // Calculate frame energy
      let energy = 0;
      for (let i = 0; i < frame.length; i++) {
        energy += frame[i] * frame[i];
      }
      energy = Math.sqrt(energy / frame.length);
      
      // Convert energy to probability (0-1)
      // Threshold based on typical speech energy levels
      const probability = Math.min(energy * 10, 1);
      probabilities.push(probability);
    }
    
    return probabilities;
  }
  
  /**
   * Find continuous speech segments from probabilities
   */
  findSpeechSegments(probabilities) {
    const segments = [];
    const threshold = this.config.speechThreshold;
    const hopSize = this.config.frameSize / 2;
    const minSpeechFrames = Math.floor(
      (this.config.minSpeechDuration * this.config.sampleRate) / hopSize
    );
    const minSilenceFrames = Math.floor(
      (this.config.minSilenceDuration * this.config.sampleRate) / hopSize
    );
    
    let inSpeech = false;
    let speechStart = 0;
    let silenceCount = 0;
    
    for (let i = 0; i < probabilities.length; i++) {
      const isSpeech = probabilities[i] > threshold;
      
      if (isSpeech) {
        if (!inSpeech) {
          // Start of speech segment
          speechStart = i;
          inSpeech = true;
        }
        silenceCount = 0;
      } else {
        if (inSpeech) {
          silenceCount++;
          
          // End speech segment if silence is long enough
          if (silenceCount >= minSilenceFrames) {
            const speechEnd = i - silenceCount;
            const duration = speechEnd - speechStart;
            
            // Only keep segments longer than minimum duration
            if (duration >= minSpeechFrames) {
              segments.push({
                start: (speechStart * hopSize) / this.config.sampleRate,
                end: (speechEnd * hopSize) / this.config.sampleRate,
                startSample: speechStart * hopSize,
                endSample: speechEnd * hopSize,
              });
            }
            
            inSpeech = false;
            silenceCount = 0;
          }
        }
      }
    }
    
    // Handle final segment
    if (inSpeech) {
      const speechEnd = probabilities.length - 1;
      const duration = speechEnd - speechStart;
      
      if (duration >= minSpeechFrames) {
        segments.push({
          start: (speechStart * hopSize) / this.config.sampleRate,
          end: (speechEnd * hopSize) / this.config.sampleRate,
          startSample: speechStart * hopSize,
          endSample: speechEnd * hopSize,
        });
      }
    }
    
    return segments;
  }
  
  /**
   * Extract speech segments from audio
   */
  extractSpeechSegments(audioData, segments) {
    if (segments.length === 0) {
      // No speech detected, return empty audio
      return new Float32Array(0);
    }
    
    // Calculate total size needed
    let totalSize = 0;
    for (const segment of segments) {
      totalSize += segment.endSample - segment.startSample;
    }
    
    // Create output array
    const output = new Float32Array(totalSize);
    let outputIndex = 0;
    
    // Copy speech segments
    for (const segment of segments) {
      const segmentData = audioData.slice(segment.startSample, segment.endSample);
      output.set(segmentData, outputIndex);
      outputIndex += segmentData.length;
    }
    
    return output;
  }
  
  /**
   * Process audio blob with VAD
   */
  async processAudioBlob(blob) {
    // Convert blob to array buffer
    const arrayBuffer = await blob.arrayBuffer();
    const audioContext = new AudioContext({ sampleRate: 16000 });
    const audioBuffer = await audioContext.decodeAudioData(arrayBuffer);
    
    // Process with VAD
    const result = await this.detectVoiceActivity(audioBuffer);
    
    // Convert processed audio back to blob
    const processedBlob = this.audioToBlob(result.processedAudio);
    
    return {
      ...result,
      blob: processedBlob,
    };
  }
  
  /**
   * Convert Float32Array to audio blob
   */
  audioToBlob(float32Array) {
    // Convert to 16-bit PCM
    const buffer = new ArrayBuffer(float32Array.length * 2);
    const view = new DataView(buffer);
    
    for (let i = 0; i < float32Array.length; i++) {
      const sample = Math.max(-1, Math.min(1, float32Array[i]));
      view.setInt16(i * 2, sample * 0x7FFF, true);
    }
    
    return new Blob([buffer], { type: 'audio/wav' });
  }
  
  /**
   * Get VAD statistics
   */
  getStats() {
    return {
      isLoaded: this.isLoaded,
      config: this.config,
      modelSize: this.model ? '~1MB' : 'Not loaded',
    };
  }
}

// Export singleton instance
export const sileroVAD = new SileroVADService();