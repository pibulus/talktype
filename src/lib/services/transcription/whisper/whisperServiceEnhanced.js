/**
 * Enhanced WhisperService with CDN support and parallel downloads
 * Implements hyperspeed model loading for 3-10x faster downloads
 */

import { get, writable } from "svelte/store";
import { userPreferences } from "../../infrastructure/stores";
import { pipeline, env } from "@xenova/transformers";
import {
  convertToWAV as convertToRawAudio,
  needsConversion,
} from "./audioConverter";
import { getModelInfo } from "./modelRegistry";
import {
  updateDownloadStatus,
  setProgress,
  setComplete,
  setError,
  updateDownloadMetrics,
  resetDownloadMetrics,
} from "./modelDownloader";
import { 
  downloadModelFast, 
  getBestCDNUrl, 
  DownloadSpeedTracker 
} from "../utils/parallelDownloader";
import { modelShareService } from "../utils/modelSharing";

// Configure transformers.js to use CDN
env.allowRemoteModels = true;
env.remoteURL = "https://cdn.jsdelivr.net/npm/@xenova/transformers@2.6.0/";
env.localURL = "/models/"; // Fallback to local if needed

// CDN configuration for models
const MODEL_CDN_CONFIG = {
  "Xenova/whisper-tiny.en": {
    onnx: "whisper-tiny-en/onnx/model.onnx",
    config: "whisper-tiny-en/config.json",
    tokenizer: "whisper-tiny-en/tokenizer.json",
  },
  "Xenova/whisper-base.en": {
    onnx: "whisper-base-en/onnx/model.onnx",
    config: "whisper-base-en/config.json",
    tokenizer: "whisper-base-en/tokenizer.json",
  },
  "Xenova/whisper-small.en": {
    onnx: "whisper-small-en/onnx/model.onnx",
    config: "whisper-small-en/config.json",
    tokenizer: "whisper-small-en/tokenizer.json",
  },
};

// Service status store
export const whisperStatus = writable({
  isLoaded: false,
  isLoading: false,
  progress: 0,
  error: null,
  selectedModel: "tiny",
  supportsWhisper: true,
  downloadSpeed: null,
  eta: null,
});

/**
 * Enhanced WhisperService with hyperspeed downloads
 */
export class WhisperServiceEnhanced {
  constructor() {
    this.transcriber = null;
    this.modelLoadPromise = null;
    this.isSupported = typeof window !== "undefined";
    this.speedTracker = new DownloadSpeedTracker();
    
    // Initialize status
    this.updateStatus({
      supportsWhisper: this.isSupported,
    });
  }

  /**
   * Update the status store with new values
   */
  updateStatus(updates) {
    whisperStatus.update((current) => ({ ...current, ...updates }));
  }

  /**
   * Preload the Whisper model with hyperspeed optimizations
   */
  async preloadModel() {
    // Don't reload if already loaded
    if (this.transcriber) {
      return { success: true, transcriber: this.transcriber };
    }

    // Return existing promise if already loading
    if (this.modelLoadPromise) {
      return this.modelLoadPromise;
    }

    // Check if running in browser environment
    if (!this.isSupported) {
      this.updateStatus({
        error: "Transformers.js is not supported in this environment",
        isLoading: false,
      });
      return { success: false, error: "Environment not supported" };
    }

    // Start the loading process
    this.updateStatus({
      isLoading: true,
      progress: 0,
      error: null,
    });
    
    resetDownloadMetrics();

    // Get selected model
    const prefs = get(userPreferences);
    const modelKey = prefs.whisperModel || "tiny";
    const modelConfig = getModelInfo(modelKey);
    
    // Update download status
    updateDownloadStatus({
      inProgress: true,
      progress: 0,
      modelId: modelKey,
      error: null,
      stage: "initializing",
    });

    // First, try to get model from another tab
    console.log("Checking for model in other tabs...");
    const sharedModel = await modelShareService.requestModel(modelConfig.id);
    
    if (sharedModel) {
      console.log("ðŸš€ Got model instantly from another tab!");
      this.updateStatus({
        progress: 100,
        selectedModel: modelKey,
      });
      setComplete();
      
      // Now we need to initialize the transcriber with the shared model
      // For now, we'll still let transformers.js handle it
      // In a full implementation, we'd load from the blob
    }

    this.modelLoadPromise = this._loadModelWithCDN(modelConfig, modelKey);
    return this.modelLoadPromise;
  }

  /**
   * Load model using CDN with parallel downloads
   */
  async _loadModelWithCDN(modelConfig, modelKey) {
    try {
      // Configure ONNX Runtime environment
      if (typeof window !== "undefined") {
        await this.configureONNXEnvironment();
      }

      this.updateStatus({
        selectedModel: modelKey,
        progress: 10,
      });

      // Override the transformers.js model loading to use our CDN
      const originalFetch = window.fetch;
      const speedTracker = this.speedTracker;
      speedTracker.start();
      
      // Track downloads for progress
      let totalBytes = 0;
      let loadedBytes = 0;
      
      // Intercept fetch requests to use our CDN and parallel downloader
      window.fetch = async function(url, options = {}) {
        // Check if this is a model file request
        if (typeof url === 'string' && url.includes('huggingface.co')) {
          console.log("Intercepting HuggingFace request:", url);
          
          // Extract the file path from the URL
          const urlParts = url.split('/');
          const fileName = urlParts[urlParts.length - 1];
          
          // Map to our CDN URL
          let cdnPath;
          if (fileName.endsWith('.onnx')) {
            cdnPath = MODEL_CDN_CONFIG[modelConfig.id]?.onnx;
          } else if (fileName === 'config.json') {
            cdnPath = MODEL_CDN_CONFIG[modelConfig.id]?.config;
          } else if (fileName === 'tokenizer.json') {
            cdnPath = MODEL_CDN_CONFIG[modelConfig.id]?.tokenizer;
          }
          
          if (cdnPath) {
            // Get the best CDN URL
            const cdnUrl = await getBestCDNUrl(cdnPath, false);
            console.log(`Using CDN: ${cdnUrl}`);
            
            // Use parallel downloader for large files
            if (fileName.endsWith('.onnx')) {
              updateDownloadStatus({ stage: "downloading" });
              
              const blob = await downloadModelFast(cdnUrl, (progress) => {
                const percentage = Math.round(progress * 80) + 10;
                whisperStatus.update(s => ({ 
                  ...s, 
                  progress: percentage,
                  downloadSpeed: speedTracker.getFormattedSpeed()
                }));
                setProgress(progress, "downloading");
                
                // Update speed metrics
                const bytesLoaded = progress * modelConfig.size;
                speedTracker.update(bytesLoaded);
                updateDownloadMetrics(bytesLoaded, modelConfig.size);
              }, 4); // Use 4 parallel chunks
              
              // Share the model with other tabs
              await modelShareService.shareModel(modelConfig.id, blob);
              
              // Return a Response object with the blob
              return new Response(blob, {
                status: 200,
                statusText: 'OK',
                headers: new Headers({
                  'content-type': 'application/octet-stream',
                  'content-length': blob.size.toString()
                })
              });
            } else {
              // For smaller files, use standard fetch with CDN
              return originalFetch.call(this, cdnUrl, options);
            }
          }
        }
        
        // Fall back to original fetch for non-model requests
        return originalFetch.call(this, url, options);
      };

      // Temporarily suppress console.warn during model loading
      const originalWarn = console.warn;
      console.warn = () => {};
      
      try {
        // Create transcription pipeline
        this.transcriber = await pipeline(
          "automatic-speech-recognition",
          modelConfig.id,
          {
            // Configure model options
            onnx: {
              logSeverityLevel: 4,
              logVerbosityLevel: 0,
              enableCpuMemArena: false,
              enableMemPattern: false,
              executionMode: "sequential",
              graphOptimizationLevel: "basic",
            },
            progress_callback: (progress) => {
              if (progress.status === "ready") {
                this.updateStatus({ progress: 95 });
                setProgress(0.95, "ready");
              }
            },
          }
        );
      } finally {
        // Restore console.warn and fetch
        console.warn = originalWarn;
        window.fetch = originalFetch;
      }

      // Model is loaded
      this.updateStatus({
        isLoaded: true,
        isLoading: false,
        progress: 100,
        error: null,
        downloadSpeed: null,
        eta: null,
      });

      // Mark download as complete
      setComplete();

      console.log("âœ¨ Whisper model loaded with HYPERSPEED!");
      return { success: true, transcriber: this.transcriber };
      
    } catch (error) {
      console.error("Failed to load Whisper model:", error);

      this.updateStatus({
        isLoaded: false,
        isLoading: false,
        progress: 0,
        error: error.message || "Failed to load Whisper model",
      });

      setError(error.message || "Failed to load Whisper model");

      this.modelLoadPromise = null;
      
      // Restore fetch if error occurred
      if (typeof window !== 'undefined' && window.fetch !== originalFetch) {
        window.fetch = originalFetch;
      }
      
      return { success: false, error };
    }
  }

  /**
   * Configure ONNX Runtime environment
   */
  async configureONNXEnvironment() {
    const waitForOrt = async () => {
      let attempts = 0;
      while (!window.ort && attempts < 10) {
        await new Promise(resolve => setTimeout(resolve, 100));
        attempts++;
      }
      
      if (window.ort) {
        try {
          window.ort.env.wasm.numThreads = 1;
          window.ort.env.logLevel = "fatal";
          window.ort.env.debug = false;
          
          if (window.ort.env.wasm) {
            window.ort.env.wasm.simd = true;
            window.ort.env.wasm.proxy = false;
          }
        } catch (e) {
          console.log("Could not configure ONNX environment:", e.message);
        }
      }
    };
    
    await waitForOrt();
  }

  /**
   * Transcribe audio using the loaded model
   */
  async transcribeAudio(audioBlob) {
    try {
      // Ensure model is loaded
      if (!this.transcriber) {
        const { success, error } = await this.preloadModel();
        if (!success) {
          throw error || new Error("Failed to load model");
        }
      }

      // Check if audio blob has content
      if (audioBlob.size === 0) {
        throw new Error("Audio blob is empty - no audio recorded");
      }

      // Convert audio to Float32Array if needed
      let processedAudio = audioBlob;
      if (needsConversion(audioBlob.type)) {
        this.updateStatus({ isLoading: true, progress: 10 });

        try {
          processedAudio = await convertToRawAudio(audioBlob);
        } catch (conversionError) {
          console.warn(
            "Audio conversion failed, using original format:",
            conversionError.message,
          );
          processedAudio = audioBlob;
        }
      }

      // Perform transcription
      this.updateStatus({ isLoading: true, progress: 20 });

      // Calculate audio duration
      let audioDuration;
      if (processedAudio instanceof Float32Array) {
        audioDuration = processedAudio.length / 16000;
      } else {
        audioDuration = processedAudio.size / (16000 * 2);
      }

      // Perform transcription with optimal configuration
      const transcriptionOptions = { task: "transcribe" };

      if (audioDuration > 15) {
        transcriptionOptions.chunk_length_s = 30;
        transcriptionOptions.stride_length_s = 5;
      }

      const result = await this.transcriber(
        processedAudio,
        transcriptionOptions,
      );

      this.updateStatus({ isLoading: false, progress: 100 });

      return result?.text || "";
      
    } catch (error) {
      console.error("Error transcribing with Whisper:", error);

      this.updateStatus({
        isLoading: false,
        error: error.message || "Failed to transcribe audio with Whisper",
      });

      throw new Error(
        `Failed to transcribe audio with Whisper: ${error.message}`,
      );
    }
  }

  /**
   * Check device capability
   */
  async checkDeviceCapability() {
    if (!this.isSupported) {
      return {
        capable: false,
        reason: "Browser environment not supported",
      };
    }

    if (navigator?.deviceMemory) {
      if (navigator.deviceMemory < 2) {
        return {
          capable: true,
          performant: false,
          reason: "Low device memory - use tiny model for best performance",
        };
      } else if (navigator.deviceMemory < 4) {
        return {
          capable: true,
          performant: true,
          reason: "Medium device memory - base model recommended",
        };
      }
    }

    return {
      capable: true,
      performant: true,
      reason: "Device appears to have sufficient resources",
    };
  }

  /**
   * Clear model from memory
   */
  unloadModel() {
    if (this.transcriber) {
      this.transcriber = null;
      this.modelLoadPromise = null;

      this.updateStatus({
        isLoaded: false,
        progress: 0,
      });

      return true;
    }
    return false;
  }
}

// Service instance
export const whisperServiceEnhanced = new WhisperServiceEnhanced();