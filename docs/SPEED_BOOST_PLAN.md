# üöÄ TalkType Hyperspeed Download Plan

_The 80/20 approach to making models download 10x faster_

## Current Situation

- Whisper Tiny: 39MB download taking ~40 seconds on average connection
- Vosk: 15MB download taking ~15 seconds
- Users getting impatient, especially on mobile
- We need SPEED, not complexity

## The Plan: 4 Simple Wins

### 1. üåç jsDelivr CDN Integration (30 mins) - **BIGGEST WIN**

Instead of serving models from our server, use jsDelivr's free global CDN:

```javascript
// OLD - Slow, single server
const modelUrl = '/models/whisper-tiny.onnx';

// NEW - Fast, 100+ global locations
const MODEL_CDN_BASE = 'https://cdn.jsdelivr.net/gh/pablomurdoch/talktype-models@v1';
const modelUrl = `${MODEL_CDN_BASE}/whisper-tiny.onnx`;
```

**Setup Steps:**

1. Create GitHub repo: `talktype-models`
2. Upload all model files
3. Create release tag `v1.0.0`
4. Update all model URLs in code
5. jsDelivr automatically caches and serves from nearest location

**Expected improvement: 3-5x faster**

### 2. ‚ö° Parallel Chunk Downloads (2 hours)

Download models in parallel chunks for massive speed boost:

```javascript
// utils/parallelDownloader.js
export async function downloadModelFast(url, onProgress) {
	const CHUNKS = 4; // Optimal for most connections

	// Get file size
	const response = await fetch(url, { method: 'HEAD' });
	const totalSize = parseInt(response.headers.get('content-length'));
	const chunkSize = Math.ceil(totalSize / CHUNKS);

	// Download all chunks in parallel
	const promises = [];
	const progress = new Array(CHUNKS).fill(0);

	for (let i = 0; i < CHUNKS; i++) {
		const start = i * chunkSize;
		const end = Math.min(start + chunkSize - 1, totalSize - 1);

		promises.push(
			fetch(url, {
				headers: { Range: `bytes=${start}-${end}` }
			}).then(async (response) => {
				const reader = response.body.getReader();
				const chunks = [];

				while (true) {
					const { done, value } = await reader.read();
					if (done) break;
					chunks.push(value);
					progress[i] += value.length;

					// Report combined progress
					if (onProgress) {
						const totalProgress = progress.reduce((a, b) => a + b, 0);
						onProgress(totalProgress / totalSize);
					}
				}

				return new Blob(chunks);
			})
		);
	}

	// Combine all chunks
	const parts = await Promise.all(promises);
	return new Blob(parts);
}
```

**Integration:**

```javascript
// In whisperService.js or voskService.js
const blob = await downloadModelFast(modelUrl, (progress) => {
	console.log(`Download: ${Math.round(progress * 100)}%`);
});
```

**Expected improvement: 2-3x faster on top of CDN**

### 3. üìª Tab-to-Tab Model Sharing (1 hour)

If user has multiple TalkType tabs open, share models between them:

```javascript
// utils/modelSharing.js
class ModelShareService {
	constructor() {
		this.channel = new BroadcastChannel('talktype-models');
		this.setupListeners();
	}

	async requestModel(modelName) {
		return new Promise((resolve) => {
			const requestId = Math.random().toString(36);

			// Listen for response
			const handler = (e) => {
				if (e.data.requestId === requestId && e.data.type === 'MODEL_DATA') {
					this.channel.removeEventListener('message', handler);
					resolve(e.data.blob);
				}
			};

			this.channel.addEventListener('message', handler);

			// Request from other tabs
			this.channel.postMessage({
				type: 'REQUEST_MODEL',
				modelName,
				requestId
			});

			// Timeout after 500ms (no other tabs)
			setTimeout(() => {
				this.channel.removeEventListener('message', handler);
				resolve(null);
			}, 500);
		});
	}

	setupListeners() {
		this.channel.addEventListener('message', async (e) => {
			if (e.data.type === 'REQUEST_MODEL') {
				// Check if we have this model in IndexedDB
				const blob = await this.getModelFromCache(e.data.modelName);
				if (blob) {
					this.channel.postMessage({
						type: 'MODEL_DATA',
						requestId: e.data.requestId,
						blob
					});
				}
			}
		});
	}

	async getModelFromCache(modelName) {
		// Check IndexedDB for model
		// Return blob if exists, null otherwise
	}
}

// Use in model loading
const sharer = new ModelShareService();
const sharedModel = await sharer.requestModel('whisper-tiny');
if (sharedModel) {
	console.log('Got model from another tab - INSTANT!');
	return sharedModel;
}
// Otherwise download normally
```

**Expected improvement: INSTANT for multi-tab users**

### 4. üóúÔ∏è Pre-Compress Models (20 mins)

Compress all models with gzip/brotli before uploading:

```bash
# One-time compression script
#!/bin/bash

# Compress with gzip (universal support)
gzip -9 -k whisper-tiny.onnx  # Creates whisper-tiny.onnx.gz

# Also create brotli version (better compression)
brotli -9 whisper-tiny.onnx    # Creates whisper-tiny.onnx.br
```

**Serve compressed versions:**

```javascript
// Try brotli first (30% better), fall back to gzip
const modelUrl = `${CDN_BASE}/whisper-tiny.onnx.br`;
const response = await fetch(modelUrl, {
	headers: { 'Accept-Encoding': 'br, gzip' }
});
// Browser auto-decompresses!
```

**Expected size reduction:**

- Whisper: 39MB ‚Üí 28MB (gzip) or 25MB (brotli)
- Vosk: 15MB ‚Üí 11MB (gzip) or 9MB (brotli)

## Implementation Order

### Tonight (3-4 hours):

1. ‚úÖ Set up jsDelivr CDN (30 mins)
2. ‚úÖ Add parallel downloader (2 hours)
3. ‚úÖ Compress and upload models (30 mins)
4. ‚úÖ Test everything (30 mins)

### Tomorrow (Optional):

5. ‚è≥ Add tab sharing (1 hour)
6. ‚è≥ Fine-tune chunk counts (30 mins)

## Expected Results

### Before:

- **Whisper**: 39MB @ 1MB/s = 39 seconds
- **Vosk**: 15MB @ 1MB/s = 15 seconds

### After (Conservative):

- **Whisper**: 25MB @ 3MB/s √ó 3 (parallel) = ~3 seconds
- **Vosk**: 9MB @ 3MB/s √ó 3 (parallel) = ~1 second

### After (Best Case - Cached CDN):

- **Whisper**: 25MB @ 10MB/s √ó 4 (parallel) = <1 second
- **Vosk**: 9MB @ 10MB/s √ó 4 (parallel) = instant

## Testing

```javascript
// Add network throttling test
async function testDownloadSpeed() {
	console.time('Download');
	const model = await downloadModelFast(modelUrl);
	console.timeEnd('Download');
	console.log('Model size:', model.size);
}

// Test with Chrome DevTools:
// 1. Network tab ‚Üí Throttling ‚Üí Fast 3G
// 2. Clear cache
// 3. Run download
// 4. Should be 5-10x faster than before
```

## Quick Wins if Lazy

Just want one thing? Do this:

```javascript
// Change this one line in your model loading:
const url = 'https://cdn.jsdelivr.net/gh/pablomurdoch/talktype-models@latest/whisper-tiny.onnx';
```

Upload models to GitHub, done. 3x speed boost minimum.

## Notes

- All solutions are **free** (jsDelivr, GitHub, compression)
- All solutions work **today** (no waiting for new tech)
- Can implement **incrementally** (each step works alone)
- **No server changes** needed (all client-side)
- Works on **all browsers** (even old ones)

---

_Remember: We're optimizing for shipping TONIGHT, not building the perfect system. Every second faster = happier users!_
